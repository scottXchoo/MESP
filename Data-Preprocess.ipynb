{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "### 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "load_path = './dataset/df_processed.parquet'\n",
    "output_dir = './dataset/divide'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True) # 출력 폴더 생성 (존재하지 않으면 생성)\n",
    "\n",
    "print(f\"Loading data from {load_path}...\")\n",
    "\n",
    "# 데이터 불러오기 & 필터링\n",
    "df_processed = pd.read_parquet(load_path, engine='fastparquet', filters=[('Trigger', '!=', 'others')], columns=['day', 'minute', 'HashApp', 'HashFunction', 'invocations'])\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "print(f\"Data shape: {df_processed.shape}\")\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 4개의 파일로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved part 1 to ./Dataset/Divide/df_part_1.parquet, shape: (222676200, 5)\n",
      "Saved part 2 to ./Dataset/Divide/df_part_2.parquet, shape: (222676200, 5)\n",
      "Saved part 3 to ./Dataset/Divide/df_part_3.parquet, shape: (222676200, 5)\n",
      "Saved part 4 to ./Dataset/Divide/df_part_4.parquet, shape: (222676200, 5)\n"
     ]
    }
   ],
   "source": [
    "# 4개의 파일로 분리하기\n",
    "num_parts = 4\n",
    "part_size = len(df_processed) // num_parts\n",
    "for i in range(num_parts):\n",
    "    start_idx = i * part_size\n",
    "    end_idx = (i + 1) * part_size if i < num_parts - 1 else len(df_processed)\n",
    "    df_part = df_processed.iloc[start_idx:end_idx]\n",
    "    part_path = f\"{output_dir}/df_part_{i+1}.parquet\"\n",
    "    \n",
    "    # Parquet 형식으로 저장\n",
    "    df_part.to_parquet(part_path, engine='pyarrow', index=False)\n",
    "    print(f\"Saved part {i+1} to {part_path}, shape: {df_part.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 재구성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Parquet files from ./dataset/divide...\n",
      "3-1: Creating FunctionID map from all files...\n",
      "Total unique functions found: 72359\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 병렬 처리로 데이터 재구성 및 저장\n",
    "\n",
    "# 입력 Parquet 파일들이 있는 디렉토리\n",
    "input_dir = './dataset/divide'\n",
    "print(f\"Reading Parquet files from {input_dir}...\") \n",
    "\n",
    "# 4개의 Parquet 파일 경로 목록\n",
    "num_parts = 4\n",
    "file_paths = [os.path.join(input_dir, f'df_part_{i+1}.parquet') for i in range(num_parts)]\n",
    "\n",
    "# 모든 파일에서 고유한 함수를 찾아 FunctionID 매핑 생성\n",
    "print(\"3-1: Creating FunctionID map from all files...\")\n",
    "function_map = {}\n",
    "next_function_id = 0\n",
    "\n",
    "for path in file_paths:\n",
    "    df_ids = pd.read_parquet(path, engine='fastparquet', columns=['HashApp', 'HashFunction'])\n",
    "\n",
    "    for app, func in df_ids.drop_duplicates().itertuples(index=False): # 중복 제거 후 반복\n",
    "        if (app, func) not in function_map:\n",
    "            function_map[(app, func)] = next_function_id\n",
    "            next_function_id += 1\n",
    "\n",
    "print(f\"Total unique functions found: {len(function_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "output_dir = './dataset/temp_results'\n",
    "os.makedirs(output_dir, exist_ok=True)  # 출력 폴더 생성 (존재하지 않으면 생성)\n",
    "\n",
    "def process_function_and_save(app_hash, func_hash, function_id, file_paths, output_dir):\n",
    "    function_dfs = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                path,\n",
    "                engine='fastparquet',\n",
    "                filters=[('HashApp', '=', app_hash), ('HashFunction', '=', func_hash)],\n",
    "                columns=['day', 'minute', 'invocations']\n",
    "            )\n",
    "            if not df.empty:\n",
    "                df['FunctionID'] = function_id\n",
    "                function_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read {path} for function {(app_hash, func_hash)}. Error: {e}\")\n",
    "    \n",
    "    if function_dfs:\n",
    "        # 이 함수에 대한 모든 데이터를 하나로 합침\n",
    "        reconstructed_df = pd.concat(function_dfs, ignore_index=True)\n",
    "\n",
    "        # 결과 DataFrame을 디스크에 저장\n",
    "        output_path = os.path.join(output_dir, f'func_{function_id}.parquet')\n",
    "        reconstructed_df.to_parquet(output_path)\n",
    "\n",
    "        # DataFrame 객체 대신 파일 경로(문자열)를 반환\n",
    "        return output_path\n",
    "    return None\n",
    "\n",
    "# 함수 단위로 데이터 읽고 재구성 후 디스크에 저장\n",
    "print(\"3-2: Reconstructing data for each function and saving to disk...\")\n",
    "unique_functions = list(function_map.keys())\n",
    "result_paths = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(process_function_and_save)(app_hash, func_hash, function_map[(app_hash, func_hash)], file_paths, output_dir)\n",
    "    for app_hash, func_hash in tqdm(unique_functions, desc=\"Processing Functions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분할 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directories created/ensured at: ./split_dataset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 원본 데이터가 있는 디렉토리\n",
    "SOURCE_DIR = './dataset/temp_results'\n",
    "\n",
    "# 분할된 데이터를 저장할 상위 디렉토리\n",
    "OUTPUT_DIR = './split_dataset'\n",
    "\n",
    "# 데이터 분할 기준 (day 컬럼 값)\n",
    "# Train: 1-8일, Validation: 9-11일, Test: 12-14일\n",
    "TRAIN_END_DAY = 8\n",
    "VALIDATION_END_DAY = 11\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "train_path = os.path.join(OUTPUT_DIR, 'train')\n",
    "val_path = os.path.join(OUTPUT_DIR, 'val')\n",
    "test_path = os.path.join(OUTPUT_DIR, 'test')\n",
    "\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(val_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "print(f\"Output directories created/ensured at: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분할 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 디렉토리에서 모든 func_...parquet 파일 목록 가져오기\n",
    "all_files = glob.glob(os.path.join(SOURCE_DIR, 'func_*.parquet'))\n",
    "\n",
    "print(f\"Found {len(all_files)} function files. Starting the split process...\")\n",
    "\n",
    "# tqdm을 사용하여 진행 상황 표시\n",
    "for file_path in tqdm(all_files, desc=\"Splitting data\"):\n",
    "    try:\n",
    "        # 함수 파일 하나를 메모리로 로드\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # 원본 파일 이름 유지\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # 'day' 컬럼을 기준으로 데이터프레임을 세 조각으로 분리\n",
    "        train_df = df[df['day'] <= TRAIN_END_DAY]\n",
    "        val_df = df[(df['day'] > TRAIN_END_DAY) & (df['day'] <= VALIDATION_END_DAY)]\n",
    "        test_df = df[df['day'] > VALIDATION_END_DAY]\n",
    "        \n",
    "        # 각 조각을 해당하는 디렉토리에 저장\n",
    "        # 데이터가 있는 경우에만 파일을 저장합니다.\n",
    "        if not train_df.empty:\n",
    "            train_df.to_parquet(os.path.join(train_path, filename), index=False)\n",
    "            \n",
    "        if not val_df.empty:\n",
    "            val_df.to_parquet(os.path.join(val_path, filename), index=False)\n",
    "\n",
    "        if not test_df.empty:\n",
    "            test_df.to_parquet(os.path.join(test_path, filename), index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {file_path}. Error: {e}\")\n",
    "\n",
    "print(\"\\n✅ Data splitting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 샘플링 및 Z-정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# TRAIN_DATA_DIR = './dataset/split_dataset/train'\n",
    "# OUTPUT_DATASET_PATH = './dataset/preprocessed_training_dataset.npy'\n",
    "\n",
    "TEST_DATA_DIR = './dataset/split_dataset/test'\n",
    "OUTPUT_DATASET_PATH = './dataset/preprocessed_test_dataset.npy'\n",
    "\n",
    "N_SAMPLES_PER_FUNCTION = 3\n",
    "ALL_DAYS = list(range(12, 15)) # Train: 1-8, Validation: 9-11, Test: 12-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 파일을 처리하는 함수\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"하나의 Parquet 파일을 읽어 4일치를 샘플링하고, wide format의 NumPy 배열로 반환\"\"\"\n",
    "    try:\n",
    "        df_func = pd.read_parquet(file_path)\n",
    "        print(f\"Processing {file_path}: initial shape {df_func.shape}\")\n",
    "\n",
    "        # 1) 중복된 (day, minute) 항목 합산\n",
    "        # 동일한 (day, minute) 조합이 여러 번 나타날 수 있으므로, 이를 합산\n",
    "        df_func_agg = df_func.groupby(['day', 'minute'], as_index=False).agg({'invocations': 'sum'})\n",
    "\n",
    "        # 2) 파일에 실제로 존재하는 날짜 확인\n",
    "        available_days = df_func['day'].unique()\n",
    "        print(f\"Available days in file: {available_days}\")\n",
    "        \n",
    "        # 3) 파일에 최소 4일치 이상의 데이터가 있는지 확인\n",
    "        if len(available_days) >= N_SAMPLES_PER_FUNCTION:\n",
    "            # 4) 존재하는 날짜 '중에서' 4일을 무작위 샘플링\n",
    "            sampled_days = np.random.choice(ALL_DAYS, size=N_SAMPLES_PER_FUNCTION, replace=False)\n",
    "            # 이제 중복이 제거된 df_func_agg에서 샘플링된 날짜만 필터링\n",
    "            df_sampled = df_func_agg[df_func_agg['day'].isin(sampled_days)]\n",
    "            print(f\"Days actually found after filtering: {df_sampled['day'].unique()}\")\n",
    "\n",
    "            # 5) 피봇 및 결측치 처리\n",
    "            # NaN 값을 0으로 채워서 불완전한 데이터를 완전하게 만듭니다.\n",
    "            # 즉, 호출 기록이 없는 minute는 0으로 간주합니다.\n",
    "            df_wide = df_sampled.pivot(index='day', columns='minute', values='invocations').fillna(0)\n",
    "\n",
    "            del df_func_agg, df_sampled, df_func  # 메모리 관리\n",
    "\n",
    "        # 6) 조건 확인 및 반환\n",
    "        if len(df_wide) == N_SAMPLES_PER_FUNCTION:\n",
    "            return df_wide.to_numpy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 오류 발생 시 상세 내용을 출력하면 디버깅에 도움이 됨\n",
    "        print(f\"Error in process_single_file for {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Sampling 3 days from each function in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1140 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2540 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4340 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6540 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9140 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12140 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 15540 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 19340 tasks      | elapsed:  1.4min\n",
      "c:\\Users\\ckh06\\anaconda3\\envs\\mesp\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 23540 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 28140 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 33140 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 38540 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 44340 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 50540 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 54800 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 58300 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 62000 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 65900 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 70632 out of 70671 | elapsed:  5.2min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 70671 out of 70671 | elapsed:  5.2min finished\n"
     ]
    }
   ],
   "source": [
    "# 병렬 처리로 샘플링 실행\n",
    "print(f\"Step 1: Sampling {N_SAMPLES_PER_FUNCTION} days from each function in parallel...\")\n",
    "all_files = glob.glob(os.path.join(TEST_DATA_DIR, 'func_*.parquet'))\n",
    "\n",
    "# n_jobs=-1: 사용 가능한 모든 CPU 코어를 사용\n",
    "# delayed: 함수와 인자를 묶어 나중에 실행하도록 예약\n",
    "results = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(process_single_file)(path) for path in all_files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 데이터셋 생성 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Concatenating all samples into a single dataset using np.memmap...\n",
      "Valid results count: 70671\n",
      "✅ Memory-mapped file created on disk. Shape: (212013, 1440)\n",
      "Filling the memory-mapped file with sampled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling data: 100%|██████████| 70671/70671 [00:00<00:00, 80602.56it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2: Concatenating all samples into a single dataset using np.memmap...\")\n",
    "\n",
    "# 결과 리스트에서 None 값을 제거하고, 유효한 결과만 필터링\n",
    "valid_results = [res for res in results if res is not None]\n",
    "print(f\"Valid results count: {len(valid_results)}\")\n",
    "\n",
    "# 빈 리스트 처리\n",
    "if not valid_results:\n",
    "    print(\"No valid patterns found. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# 최종 데이터셋의 형태(shape) 계산\n",
    "num_samples = sum(res.shape[0] for res in valid_results)\n",
    "num_timesteps = valid_results[0].shape[1]  # 각 샘플의 길이 (1440)\n",
    "final_shape = (num_samples, num_timesteps)\n",
    "\n",
    "# 1) 디스크에 최종 배열을 위한 공간 미리 할당 (메모리 사용 거의 없음)\n",
    "memmap_path = './dataset/final_dataset_raw.mmap'\n",
    "final_dataset_raw = np.memmap(memmap_path, dtype='float32', mode='w+', shape=final_shape)\n",
    "print(f\"✅ Memory-mapped file created on disk. Shape: {final_shape}\") # (282684, 1440)\n",
    "\n",
    "# 2) 작은 조각들을 순차적으로 디스크의 배열에 채워 넣기\n",
    "print(\"Filling the memory-mapped file with sampled data...\")\n",
    "start_idx = 0\n",
    "for res in tqdm(valid_results, desc=\"Filling data\"):\n",
    "    num_rows_in_chunk = res.shape[0]\n",
    "    # RAM에는 작은 조각(res)만 있고, 디스크의 큰 배열(final_dataset_raw)에 복사\n",
    "    final_dataset_raw[start_idx : start_idx + num_rows_in_chunk] = res\n",
    "    start_idx += num_rows_in_chunk\n",
    "\n",
    "# 디스크에 변경 사항 최종 저장\n",
    "final_dataset_raw.flush()\n",
    "del valid_results  # 메모리 해제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-정규화 및 최종 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Applying Z-Normalization and saving the final dataset...\n",
      "Applying Z-Scaler...\n",
      "Reshaping data for tslearn models...\n",
      "Saving the final preprocessed dataset to ./dataset/preprocessed_test_dataset.npy...\n",
      "\n",
      "🎉 All preprocessing is complete!\n",
      "Final dataset is ready for training at: ./dataset/preprocessed_test_dataset.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 3: Applying Z-Normalization and saving the final dataset...\")\n",
    "\n",
    "# 1) 메모리 맵 파일 다시 로드 (읽기 전용)\n",
    "# 이전 단계에서 생성된 메모리 맵 파일 경로 및 정보\n",
    "memmap_path = './dataset/final_dataset_raw.mmap'\n",
    "dtype = 'float32'\n",
    "\n",
    "# 2) Z-정규화\n",
    "# mode='r'은 읽기 전용으로 파일을 엽니다.\n",
    "raw_data_mmap = np.memmap(memmap_path, dtype=dtype, mode='r', shape=final_shape)\n",
    "\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)\n",
    "print(\"Applying Z-Scaler...\")\n",
    "final_dataset_normalized = scaler.fit_transform(raw_data_mmap)\n",
    "\n",
    "# 3) tslearn 모델 입력을 위한 3D 형태로 변환\n",
    "# 이 연산은 데이터 복사본을 생성하므로 충분한 메모리가 필요할 수 있습니다.\n",
    "print(\"Reshaping data for tslearn models...\")\n",
    "final_dataset_ready = final_dataset_normalized.reshape(\n",
    "    final_dataset_normalized.shape[0], \n",
    "    final_dataset_normalized.shape[1], \n",
    "    1\n",
    ")\n",
    "\n",
    "# 4) 최종 결과물(.npy) 저장\n",
    "print(f\"Saving the final preprocessed dataset to {OUTPUT_DATASET_PATH}...\")\n",
    "np.save(OUTPUT_DATASET_PATH, final_dataset_ready)\n",
    "\n",
    "# 5) 메모리 매핑 파일 핸들 해제 및 임시 파일 삭제\n",
    "del raw_data_mmap, final_dataset_normalized, final_dataset_ready # 메모리에서 변수 해제\n",
    "\n",
    "print(\"\\n🎉 All preprocessing is complete!\")\n",
    "print(f\"Final dataset is ready for training at: {OUTPUT_DATASET_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
