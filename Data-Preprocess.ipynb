{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬\n",
    "### 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "load_path = './dataset/df_processed.parquet'\n",
    "output_dir = './dataset/divide'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True) # ì¶œë ¥ í´ë” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±)\n",
    "\n",
    "print(f\"Loading data from {load_path}...\")\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° & í•„í„°ë§\n",
    "df_processed = pd.read_parquet(load_path, engine='fastparquet', filters=[('Trigger', '!=', 'others')], columns=['day', 'minute', 'HashApp', 'HashFunction', 'invocations'])\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "print(f\"Data shape: {df_processed.shape}\")\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 4ê°œì˜ íŒŒì¼ë¡œ ë¶„ë¦¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved part 1 to ./Dataset/Divide/df_part_1.parquet, shape: (222676200, 5)\n",
      "Saved part 2 to ./Dataset/Divide/df_part_2.parquet, shape: (222676200, 5)\n",
      "Saved part 3 to ./Dataset/Divide/df_part_3.parquet, shape: (222676200, 5)\n",
      "Saved part 4 to ./Dataset/Divide/df_part_4.parquet, shape: (222676200, 5)\n"
     ]
    }
   ],
   "source": [
    "# 4ê°œì˜ íŒŒì¼ë¡œ ë¶„ë¦¬í•˜ê¸°\n",
    "num_parts = 4\n",
    "part_size = len(df_processed) // num_parts\n",
    "for i in range(num_parts):\n",
    "    start_idx = i * part_size\n",
    "    end_idx = (i + 1) * part_size if i < num_parts - 1 else len(df_processed)\n",
    "    df_part = df_processed.iloc[start_idx:end_idx]\n",
    "    part_path = f\"{output_dir}/df_part_{i+1}.parquet\"\n",
    "    \n",
    "    # Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "    df_part.to_parquet(part_path, engine='pyarrow', index=False)\n",
    "    print(f\"Saved part {i+1} to {part_path}, shape: {df_part.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ë°ì´í„° ì¬êµ¬ì„± ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Parquet files from ./dataset/divide...\n",
      "3-1: Creating FunctionID map from all files...\n",
      "Total unique functions found: 72359\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë°ì´í„° ì¬êµ¬ì„± ë° ì €ì¥\n",
    "\n",
    "# ì…ë ¥ Parquet íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
    "input_dir = './dataset/divide'\n",
    "print(f\"Reading Parquet files from {input_dir}...\") \n",
    "\n",
    "# 4ê°œì˜ Parquet íŒŒì¼ ê²½ë¡œ ëª©ë¡\n",
    "num_parts = 4\n",
    "file_paths = [os.path.join(input_dir, f'df_part_{i+1}.parquet') for i in range(num_parts)]\n",
    "\n",
    "# ëª¨ë“  íŒŒì¼ì—ì„œ ê³ ìœ í•œ í•¨ìˆ˜ë¥¼ ì°¾ì•„ FunctionID ë§¤í•‘ ìƒì„±\n",
    "print(\"3-1: Creating FunctionID map from all files...\")\n",
    "function_map = {}\n",
    "next_function_id = 0\n",
    "\n",
    "for path in file_paths:\n",
    "    df_ids = pd.read_parquet(path, engine='fastparquet', columns=['HashApp', 'HashFunction'])\n",
    "\n",
    "    for app, func in df_ids.drop_duplicates().itertuples(index=False): # ì¤‘ë³µ ì œê±° í›„ ë°˜ë³µ\n",
    "        if (app, func) not in function_map:\n",
    "            function_map[(app, func)] = next_function_id\n",
    "            next_function_id += 1\n",
    "\n",
    "print(f\"Total unique functions found: {len(function_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "output_dir = './dataset/temp_results'\n",
    "os.makedirs(output_dir, exist_ok=True)  # ì¶œë ¥ í´ë” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±)\n",
    "\n",
    "def process_function_and_save(app_hash, func_hash, function_id, file_paths, output_dir):\n",
    "    function_dfs = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_parquet(\n",
    "                path,\n",
    "                engine='fastparquet',\n",
    "                filters=[('HashApp', '=', app_hash), ('HashFunction', '=', func_hash)],\n",
    "                columns=['day', 'minute', 'invocations']\n",
    "            )\n",
    "            if not df.empty:\n",
    "                df['FunctionID'] = function_id\n",
    "                function_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read {path} for function {(app_hash, func_hash)}. Error: {e}\")\n",
    "    \n",
    "    if function_dfs:\n",
    "        # ì´ í•¨ìˆ˜ì— ëŒ€í•œ ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨\n",
    "        reconstructed_df = pd.concat(function_dfs, ignore_index=True)\n",
    "\n",
    "        # ê²°ê³¼ DataFrameì„ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "        output_path = os.path.join(output_dir, f'func_{function_id}.parquet')\n",
    "        reconstructed_df.to_parquet(output_path)\n",
    "\n",
    "        # DataFrame ê°ì²´ ëŒ€ì‹  íŒŒì¼ ê²½ë¡œ(ë¬¸ìì—´)ë¥¼ ë°˜í™˜\n",
    "        return output_path\n",
    "    return None\n",
    "\n",
    "# í•¨ìˆ˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì½ê³  ì¬êµ¬ì„± í›„ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "print(\"3-2: Reconstructing data for each function and saving to disk...\")\n",
    "unique_functions = list(function_map.keys())\n",
    "result_paths = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(process_function_and_save)(app_hash, func_hash, function_map[(app_hash, func_hash)], file_paths, output_dir)\n",
    "    for app_hash, func_hash in tqdm(unique_functions, desc=\"Processing Functions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¶„í•  ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directories created/ensured at: ./split_dataset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
    "SOURCE_DIR = './dataset/temp_results'\n",
    "\n",
    "# ë¶„í• ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ìƒìœ„ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = './split_dataset'\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  ê¸°ì¤€ (day ì»¬ëŸ¼ ê°’)\n",
    "# Train: 1-8ì¼, Validation: 9-11ì¼, Test: 12-14ì¼\n",
    "TRAIN_END_DAY = 8\n",
    "VALIDATION_END_DAY = 11\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "train_path = os.path.join(OUTPUT_DIR, 'train')\n",
    "val_path = os.path.join(OUTPUT_DIR, 'val')\n",
    "test_path = os.path.join(OUTPUT_DIR, 'test')\n",
    "\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(val_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "print(f\"Output directories created/ensured at: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ë¶„í•  ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  func_...parquet íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "all_files = glob.glob(os.path.join(SOURCE_DIR, 'func_*.parquet'))\n",
    "\n",
    "print(f\"Found {len(all_files)} function files. Starting the split process...\")\n",
    "\n",
    "# tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "for file_path in tqdm(all_files, desc=\"Splitting data\"):\n",
    "    try:\n",
    "        # í•¨ìˆ˜ íŒŒì¼ í•˜ë‚˜ë¥¼ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œ\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # ì›ë³¸ íŒŒì¼ ì´ë¦„ ìœ ì§€\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # 'day' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ì„¸ ì¡°ê°ìœ¼ë¡œ ë¶„ë¦¬\n",
    "        train_df = df[df['day'] <= TRAIN_END_DAY]\n",
    "        val_df = df[(df['day'] > TRAIN_END_DAY) & (df['day'] <= VALIDATION_END_DAY)]\n",
    "        test_df = df[df['day'] > VALIDATION_END_DAY]\n",
    "        \n",
    "        # ê° ì¡°ê°ì„ í•´ë‹¹í•˜ëŠ” ë””ë ‰í† ë¦¬ì— ì €ì¥\n",
    "        # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        if not train_df.empty:\n",
    "            train_df.to_parquet(os.path.join(train_path, filename), index=False)\n",
    "            \n",
    "        if not val_df.empty:\n",
    "            val_df.to_parquet(os.path.join(val_path, filename), index=False)\n",
    "\n",
    "        if not test_df.empty:\n",
    "            test_df.to_parquet(os.path.join(test_path, filename), index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {file_path}. Error: {e}\")\n",
    "\n",
    "print(\"\\nâœ… Data splitting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ìƒ˜í”Œë§ ë° Z-ì •ê·œí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì •\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# TRAIN_DATA_DIR = './dataset/split_dataset/train'\n",
    "# OUTPUT_DATASET_PATH = './dataset/preprocessed_training_dataset.npy'\n",
    "\n",
    "TEST_DATA_DIR = './dataset/split_dataset/test'\n",
    "OUTPUT_DATASET_PATH = './dataset/preprocessed_test_dataset.npy'\n",
    "\n",
    "N_SAMPLES_PER_FUNCTION = 3\n",
    "ALL_DAYS = list(range(12, 15)) # Train: 1-8, Validation: 9-11, Test: 12-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def process_single_file(file_path):\n",
    "    \"\"\"í•˜ë‚˜ì˜ Parquet íŒŒì¼ì„ ì½ì–´ 4ì¼ì¹˜ë¥¼ ìƒ˜í”Œë§í•˜ê³ , wide formatì˜ NumPy ë°°ì—´ë¡œ ë°˜í™˜\"\"\"\n",
    "    try:\n",
    "        df_func = pd.read_parquet(file_path)\n",
    "        print(f\"Processing {file_path}: initial shape {df_func.shape}\")\n",
    "\n",
    "        # 1) ì¤‘ë³µëœ (day, minute) í•­ëª© í•©ì‚°\n",
    "        # ë™ì¼í•œ (day, minute) ì¡°í•©ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ í•©ì‚°\n",
    "        df_func_agg = df_func.groupby(['day', 'minute'], as_index=False).agg({'invocations': 'sum'})\n",
    "\n",
    "        # 2) íŒŒì¼ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë‚ ì§œ í™•ì¸\n",
    "        available_days = df_func['day'].unique()\n",
    "        print(f\"Available days in file: {available_days}\")\n",
    "        \n",
    "        # 3) íŒŒì¼ì— ìµœì†Œ 4ì¼ì¹˜ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if len(available_days) >= N_SAMPLES_PER_FUNCTION:\n",
    "            # 4) ì¡´ì¬í•˜ëŠ” ë‚ ì§œ 'ì¤‘ì—ì„œ' 4ì¼ì„ ë¬´ì‘ìœ„ ìƒ˜í”Œë§\n",
    "            sampled_days = np.random.choice(ALL_DAYS, size=N_SAMPLES_PER_FUNCTION, replace=False)\n",
    "            # ì´ì œ ì¤‘ë³µì´ ì œê±°ëœ df_func_aggì—ì„œ ìƒ˜í”Œë§ëœ ë‚ ì§œë§Œ í•„í„°ë§\n",
    "            df_sampled = df_func_agg[df_func_agg['day'].isin(sampled_days)]\n",
    "            print(f\"Days actually found after filtering: {df_sampled['day'].unique()}\")\n",
    "\n",
    "            # 5) í”¼ë´‡ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "            # NaN ê°’ì„ 0ìœ¼ë¡œ ì±„ì›Œì„œ ë¶ˆì™„ì „í•œ ë°ì´í„°ë¥¼ ì™„ì „í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "            # ì¦‰, í˜¸ì¶œ ê¸°ë¡ì´ ì—†ëŠ” minuteëŠ” 0ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\n",
    "            df_wide = df_sampled.pivot(index='day', columns='minute', values='invocations').fillna(0)\n",
    "\n",
    "            del df_func_agg, df_sampled, df_func  # ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "\n",
    "        # 6) ì¡°ê±´ í™•ì¸ ë° ë°˜í™˜\n",
    "        if len(df_wide) == N_SAMPLES_PER_FUNCTION:\n",
    "            return df_wide.to_numpy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒì„¸ ë‚´ìš©ì„ ì¶œë ¥í•˜ë©´ ë””ë²„ê¹…ì— ë„ì›€ì´ ë¨\n",
    "        print(f\"Error in process_single_file for {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Sampling 3 days from each function in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1140 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2540 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4340 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6540 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9140 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 12140 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 15540 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 19340 tasks      | elapsed:  1.4min\n",
      "c:\\Users\\ckh06\\anaconda3\\envs\\mesp\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 23540 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 28140 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 33140 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 38540 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 44340 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 50540 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 54800 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 58300 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 62000 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 65900 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 70632 out of 70671 | elapsed:  5.2min remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 70671 out of 70671 | elapsed:  5.2min finished\n"
     ]
    }
   ],
   "source": [
    "# ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìƒ˜í”Œë§ ì‹¤í–‰\n",
    "print(f\"Step 1: Sampling {N_SAMPLES_PER_FUNCTION} days from each function in parallel...\")\n",
    "all_files = glob.glob(os.path.join(TEST_DATA_DIR, 'func_*.parquet'))\n",
    "\n",
    "# n_jobs=-1: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©\n",
    "# delayed: í•¨ìˆ˜ì™€ ì¸ìë¥¼ ë¬¶ì–´ ë‚˜ì¤‘ì— ì‹¤í–‰í•˜ë„ë¡ ì˜ˆì•½\n",
    "results = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(process_single_file)(path) for path in all_files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Concatenating all samples into a single dataset using np.memmap...\n",
      "Valid results count: 70671\n",
      "âœ… Memory-mapped file created on disk. Shape: (212013, 1440)\n",
      "Filling the memory-mapped file with sampled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70671/70671 [00:00<00:00, 80602.56it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2: Concatenating all samples into a single dataset using np.memmap...\")\n",
    "\n",
    "# ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ None ê°’ì„ ì œê±°í•˜ê³ , ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§\n",
    "valid_results = [res for res in results if res is not None]\n",
    "print(f\"Valid results count: {len(valid_results)}\")\n",
    "\n",
    "# ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "if not valid_results:\n",
    "    print(\"No valid patterns found. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# ìµœì¢… ë°ì´í„°ì…‹ì˜ í˜•íƒœ(shape) ê³„ì‚°\n",
    "num_samples = sum(res.shape[0] for res in valid_results)\n",
    "num_timesteps = valid_results[0].shape[1]  # ê° ìƒ˜í”Œì˜ ê¸¸ì´ (1440)\n",
    "final_shape = (num_samples, num_timesteps)\n",
    "\n",
    "# 1) ë””ìŠ¤í¬ì— ìµœì¢… ë°°ì—´ì„ ìœ„í•œ ê³µê°„ ë¯¸ë¦¬ í• ë‹¹ (ë©”ëª¨ë¦¬ ì‚¬ìš© ê±°ì˜ ì—†ìŒ)\n",
    "memmap_path = './dataset/final_dataset_raw.mmap'\n",
    "final_dataset_raw = np.memmap(memmap_path, dtype='float32', mode='w+', shape=final_shape)\n",
    "print(f\"âœ… Memory-mapped file created on disk. Shape: {final_shape}\") # (282684, 1440)\n",
    "\n",
    "# 2) ì‘ì€ ì¡°ê°ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ë””ìŠ¤í¬ì˜ ë°°ì—´ì— ì±„ì›Œ ë„£ê¸°\n",
    "print(\"Filling the memory-mapped file with sampled data...\")\n",
    "start_idx = 0\n",
    "for res in tqdm(valid_results, desc=\"Filling data\"):\n",
    "    num_rows_in_chunk = res.shape[0]\n",
    "    # RAMì—ëŠ” ì‘ì€ ì¡°ê°(res)ë§Œ ìˆê³ , ë””ìŠ¤í¬ì˜ í° ë°°ì—´(final_dataset_raw)ì— ë³µì‚¬\n",
    "    final_dataset_raw[start_idx : start_idx + num_rows_in_chunk] = res\n",
    "    start_idx += num_rows_in_chunk\n",
    "\n",
    "# ë””ìŠ¤í¬ì— ë³€ê²½ ì‚¬í•­ ìµœì¢… ì €ì¥\n",
    "final_dataset_raw.flush()\n",
    "del valid_results  # ë©”ëª¨ë¦¬ í•´ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-ì •ê·œí™” ë° ìµœì¢… ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Applying Z-Normalization and saving the final dataset...\n",
      "Applying Z-Scaler...\n",
      "Reshaping data for tslearn models...\n",
      "Saving the final preprocessed dataset to ./dataset/preprocessed_test_dataset.npy...\n",
      "\n",
      "ğŸ‰ All preprocessing is complete!\n",
      "Final dataset is ready for training at: ./dataset/preprocessed_test_dataset.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 3: Applying Z-Normalization and saving the final dataset...\")\n",
    "\n",
    "# 1) ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ë‹¤ì‹œ ë¡œë“œ (ì½ê¸° ì „ìš©)\n",
    "# ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ê²½ë¡œ ë° ì •ë³´\n",
    "memmap_path = './dataset/final_dataset_raw.mmap'\n",
    "dtype = 'float32'\n",
    "\n",
    "# 2) Z-ì •ê·œí™”\n",
    "# mode='r'ì€ ì½ê¸° ì „ìš©ìœ¼ë¡œ íŒŒì¼ì„ ì—½ë‹ˆë‹¤.\n",
    "raw_data_mmap = np.memmap(memmap_path, dtype=dtype, mode='r', shape=final_shape)\n",
    "\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)\n",
    "print(\"Applying Z-Scaler...\")\n",
    "final_dataset_normalized = scaler.fit_transform(raw_data_mmap)\n",
    "\n",
    "# 3) tslearn ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ 3D í˜•íƒœë¡œ ë³€í™˜\n",
    "# ì´ ì—°ì‚°ì€ ë°ì´í„° ë³µì‚¬ë³¸ì„ ìƒì„±í•˜ë¯€ë¡œ ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "print(\"Reshaping data for tslearn models...\")\n",
    "final_dataset_ready = final_dataset_normalized.reshape(\n",
    "    final_dataset_normalized.shape[0], \n",
    "    final_dataset_normalized.shape[1], \n",
    "    1\n",
    ")\n",
    "\n",
    "# 4) ìµœì¢… ê²°ê³¼ë¬¼(.npy) ì €ì¥\n",
    "print(f\"Saving the final preprocessed dataset to {OUTPUT_DATASET_PATH}...\")\n",
    "np.save(OUTPUT_DATASET_PATH, final_dataset_ready)\n",
    "\n",
    "# 5) ë©”ëª¨ë¦¬ ë§¤í•‘ íŒŒì¼ í•¸ë“¤ í•´ì œ ë° ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
    "del raw_data_mmap, final_dataset_normalized, final_dataset_ready # ë©”ëª¨ë¦¬ì—ì„œ ë³€ìˆ˜ í•´ì œ\n",
    "\n",
    "print(\"\\nğŸ‰ All preprocessing is complete!\")\n",
    "print(f\"Final dataset is ready for training at: {OUTPUT_DATASET_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
